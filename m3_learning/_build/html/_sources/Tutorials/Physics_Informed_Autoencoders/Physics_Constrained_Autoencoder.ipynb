{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxrwNCW3d_W5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Physics Constrained Autoencoders\n",
    "\n",
    "By Mary Ye$^1$, Joshua C. Agar$^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxrwNCW3d_W5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$^1$ Department of Computer science and Engineering, Lehigh University\n",
    "$^2$ Department of Mechanical Engineering and Mechanics, Drexel University\n",
    "\n",
    "- There are many times where you want to fit spectroscopic data to a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Classical fitting methods can be used but break down:\n",
    "  - When data is noisy\n",
    "  - There are multiple candidate models\n",
    "  - Data is high velocity\n",
    "  - Data is noisy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oWjlBBJzs73",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imports Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: m3_learning in c:\\conda\\envs\\m3_learning\\lib\\site-packages (0.0.5)\n",
      "Requirement already satisfied: torchsummary in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from m3_learning) (1.5.1)\n",
      "Requirement already satisfied: matplotlib==3.6.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from m3_learning) (3.6.1)\n",
      "Requirement already satisfied: setuptools==65.5.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from m3_learning) (65.5.0)\n",
      "Requirement already satisfied: tqdm==4.64.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from m3_learning) (4.64.1)\n",
      "Requirement already satisfied: jupyter-book in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from m3_learning) (0.13.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from m3_learning) (1.1.2)\n",
      "Requirement already satisfied: torch in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from m3_learning) (1.12.1)\n",
      "Requirement already satisfied: scikit-image in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from m3_learning) (0.19.3)\n",
      "Requirement already satisfied: numpy==1.23.4 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from m3_learning) (1.23.4)\n",
      "Requirement already satisfied: opencv-python in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from m3_learning) (4.6.0.66)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from m3_learning) (8.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from matplotlib==3.6.1->m3_learning) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from matplotlib==3.6.1->m3_learning) (4.37.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from matplotlib==3.6.1->m3_learning) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from matplotlib==3.6.1->m3_learning) (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from matplotlib==3.6.1->m3_learning) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from matplotlib==3.6.1->m3_learning) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from matplotlib==3.6.1->m3_learning) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from matplotlib==3.6.1->m3_learning) (21.3)\n",
      "Requirement already satisfied: colorama in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from tqdm==4.64.1->m3_learning) (0.4.5)\n",
      "Requirement already satisfied: click<9,>=7.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (8.1.3)\n",
      "Requirement already satisfied: sphinx_togglebutton in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (0.3.2)\n",
      "Requirement already satisfied: pyyaml in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (6.0)\n",
      "Requirement already satisfied: sphinx-copybutton in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (0.5.0)\n",
      "Requirement already satisfied: sphinx_book_theme~=0.3.2 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (0.3.3)\n",
      "Requirement already satisfied: jsonschema<4 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (3.2.0)\n",
      "Requirement already satisfied: sphinx-jupyterbook-latex~=0.4.6 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (0.4.7)\n",
      "Requirement already satisfied: linkify-it-py~=1.0.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (1.0.3)\n",
      "Requirement already satisfied: myst-nb~=0.13.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (0.13.2)\n",
      "Requirement already satisfied: sphinxcontrib-bibtex<=2.5.0,>=2.2.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (2.5.0)\n",
      "Requirement already satisfied: sphinx-comments in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (0.0.3)\n",
      "Requirement already satisfied: Jinja2 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (3.1.2)\n",
      "Requirement already satisfied: sphinx-thebe~=0.1.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (0.1.2)\n",
      "Requirement already satisfied: sphinx-multitoc-numbering~=0.1.3 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (0.1.3)\n",
      "Requirement already satisfied: sphinx-design~=0.1.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (0.1.0)\n",
      "Requirement already satisfied: sphinx<5,>=4 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (4.5.0)\n",
      "Requirement already satisfied: sphinx-external-toc~=0.2.3 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (0.2.4)\n",
      "Requirement already satisfied: docutils<0.18,>=0.15 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-book->m3_learning) (0.17.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from scikit-image->m3_learning) (2.22.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from scikit-image->m3_learning) (1.4.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from scikit-image->m3_learning) (2022.10.10)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from scikit-image->m3_learning) (2.8.7)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from scikit-image->m3_learning) (1.9.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from scikit-learn->m3_learning) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from scikit-learn->m3_learning) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from torch->m3_learning) (4.3.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jsonschema<4->jupyter-book->m3_learning) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jsonschema<4->jupyter-book->m3_learning) (0.18.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jsonschema<4->jupyter-book->m3_learning) (1.16.0)\n",
      "Requirement already satisfied: uc-micro-py in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from linkify-it-py~=1.0.1->jupyter-book->m3_learning) (1.0.1)\n",
      "Requirement already satisfied: jupyter-sphinx~=0.3.2 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from myst-nb~=0.13.1->jupyter-book->m3_learning) (0.3.2)\n",
      "Requirement already satisfied: ipywidgets<8,>=7.0.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from myst-nb~=0.13.1->jupyter-book->m3_learning) (7.7.2)\n",
      "Requirement already satisfied: ipython in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from myst-nb~=0.13.1->jupyter-book->m3_learning) (8.4.0)\n",
      "Requirement already satisfied: myst-parser~=0.15.2 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from myst-nb~=0.13.1->jupyter-book->m3_learning) (0.15.2)\n",
      "Requirement already satisfied: nbformat~=5.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from myst-nb~=0.13.1->jupyter-book->m3_learning) (5.7.0)\n",
      "Requirement already satisfied: jupyter-cache~=0.4.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from myst-nb~=0.13.1->jupyter-book->m3_learning) (0.4.3)\n",
      "Requirement already satisfied: importlib-metadata in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from myst-nb~=0.13.1->jupyter-book->m3_learning) (5.0.0)\n",
      "Requirement already satisfied: nbconvert<7,>=5.6 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from myst-nb~=0.13.1->jupyter-book->m3_learning) (6.5.4)\n",
      "Requirement already satisfied: Pygments>=2.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinx<5,>=4->jupyter-book->m3_learning) (2.11.2)\n",
      "Requirement already satisfied: babel>=1.3 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinx<5,>=4->jupyter-book->m3_learning) (2.10.3)\n",
      "Requirement already satisfied: imagesize in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinx<5,>=4->jupyter-book->m3_learning) (1.4.1)\n",
      "Requirement already satisfied: requests>=2.5.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinx<5,>=4->jupyter-book->m3_learning) (2.28.1)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinx<5,>=4->jupyter-book->m3_learning) (1.0.2)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinx<5,>=4->jupyter-book->m3_learning) (1.0.3)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinx<5,>=4->jupyter-book->m3_learning) (0.7.12)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinx<5,>=4->jupyter-book->m3_learning) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinx<5,>=4->jupyter-book->m3_learning) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinx<5,>=4->jupyter-book->m3_learning) (1.0.2)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinx<5,>=4->jupyter-book->m3_learning) (1.1.5)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinx<5,>=4->jupyter-book->m3_learning) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from Jinja2->jupyter-book->m3_learning) (2.1.1)\n",
      "Requirement already satisfied: pydata-sphinx-theme~=0.8.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinx_book_theme~=0.3.2->jupyter-book->m3_learning) (0.8.1)\n",
      "Requirement already satisfied: wheel in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinx_togglebutton->jupyter-book->m3_learning) (0.37.1)\n",
      "Requirement already satisfied: pybtex-docutils>=1.0.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinxcontrib-bibtex<=2.5.0,>=2.2.0->jupyter-book->m3_learning) (1.0.2)\n",
      "Requirement already satisfied: pybtex>=0.24 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sphinxcontrib-bibtex<=2.5.0,>=2.2.0->jupyter-book->m3_learning) (0.24.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from babel>=1.3->sphinx<5,>=4->jupyter-book->m3_learning) (2022.4)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (3.6.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (5.1.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (1.1.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (6.15.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipython->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipython->myst-nb~=0.13.1->jupyter-book->m3_learning) (3.0.20)\n",
      "Requirement already satisfied: decorator in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipython->myst-nb~=0.13.1->jupyter-book->m3_learning) (5.1.1)\n",
      "Requirement already satisfied: backcall in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipython->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipython->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipython->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.7.5)\n",
      "Requirement already satisfied: stack-data in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipython->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.2.0)\n",
      "Requirement already satisfied: sqlalchemy<1.5,>=1.3.12 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (1.4.41)\n",
      "Requirement already satisfied: nbdime in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (3.1.1)\n",
      "Requirement already satisfied: nbclient<0.6,>=0.2 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.5.13)\n",
      "Requirement already satisfied: mdit-py-plugins~=0.2.8 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from myst-parser~=0.15.2->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.2.8)\n",
      "Requirement already satisfied: markdown-it-py<2.0.0,>=1.0.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from myst-parser~=0.15.2->myst-nb~=0.13.1->jupyter-book->m3_learning) (1.1.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.2.2)\n",
      "Requirement already satisfied: tinycss2 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book->m3_learning) (1.1.1)\n",
      "Requirement already satisfied: lxml in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book->m3_learning) (4.9.1)\n",
      "Requirement already satisfied: defusedxml in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.7.1)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book->m3_learning) (4.11.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book->m3_learning) (4.11.1)\n",
      "Requirement already satisfied: bleach in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book->m3_learning) (5.0.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book->m3_learning) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.4)\n",
      "Requirement already satisfied: fastjsonschema in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbformat~=5.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (2.16.2)\n",
      "Requirement already satisfied: latexcodec>=1.0.4 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from pybtex>=0.24->sphinxcontrib-bibtex<=2.5.0,>=2.2.0->jupyter-book->m3_learning) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from requests>=2.5.0->sphinx<5,>=4->jupyter-book->m3_learning) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from requests>=2.5.0->sphinx<5,>=4->jupyter-book->m3_learning) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from requests>=2.5.0->sphinx<5,>=4->jupyter-book->m3_learning) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from requests>=2.5.0->sphinx<5,>=4->jupyter-book->m3_learning) (2.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from importlib-metadata->myst-nb~=0.13.1->jupyter-book->m3_learning) (3.9.0)\n",
      "Requirement already satisfied: nest-asyncio in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (1.5.5)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (23.2.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (7.3.5)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (6.2)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (1.5.1)\n",
      "Requirement already satisfied: psutil in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (5.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jedi>=0.16->ipython->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.8.3)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-core>=4.7->nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book->m3_learning) (302)\n",
      "Requirement already satisfied: wcwidth in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.2.5)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from sqlalchemy<1.5,>=1.3.12->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (1.1.3.post0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from widgetsnbextension~=3.6.0->ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (6.5.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from beautifulsoup4->nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book->m3_learning) (2.3.2.post1)\n",
      "Requirement already satisfied: webencodings in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from bleach->nbconvert<7,>=5.6->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.5.1)\n",
      "Requirement already satisfied: GitPython!=2.1.4,!=2.1.5,!=2.1.6 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (3.1.29)\n",
      "Requirement already satisfied: jupyter-server in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (1.21.0)\n",
      "Requirement already satisfied: jupyter-server-mathjax>=0.2.2 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.2.6)\n",
      "Requirement already satisfied: asttokens in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from stack-data->ipython->myst-nb~=0.13.1->jupyter-book->m3_learning) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from stack-data->ipython->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.2.2)\n",
      "Requirement already satisfied: executing in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from stack-data->ipython->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.8.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from GitPython!=2.1.4,!=2.1.5,!=2.1.6->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (4.0.9)\n",
      "Requirement already satisfied: websocket-client in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (1.4.1)\n",
      "Requirement already satisfied: pywinpty in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (2.0.8)\n",
      "Requirement already satisfied: prometheus-client in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.15.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.16.0)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (3.6.1)\n",
      "Requirement already satisfied: Send2Trash in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (21.3.0)\n",
      "Requirement already satisfied: nbclassic==0.4.5 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.4.5)\n",
      "Requirement already satisfied: notebook-shim>=0.1.0 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from nbclassic==0.4.5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7.0.0->myst-nb~=0.13.1->jupyter-book->m3_learning) (0.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from anyio<4,>=3.1.0->jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=2.1.4,!=2.1.5,!=2.1.6->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (5.0.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from argon2-cffi->jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\conda\\envs\\m3_learning\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server->nbdime->jupyter-cache~=0.4.1->myst-nb~=0.13.1->jupyter-book->m3_learning) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install m3_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0VKiyka604-",
    "outputId": "272c5a5b-36ac-4db0-b3d9-7b88401a9e13",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import math\n",
    "\n",
    "from m3_learning.nn.time_series_nn.nn_util import Train, transform_nn\n",
    "from m3_learning.viz.layout import layout_fig, embedding_maps, latent_generator\n",
    "from m3_learning.util.rand_util import rand_tensor, set_seeds\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "plt.rcParams.update({\"xtick.direction\": \"in\", \"ytick.direction\": \"in\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8x-NFM-D0hpm",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generating some data based on the image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8x-NFM-D0hpm",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Define a non-linear function\n",
    "\n",
    "<center> $$ y = A sin(2\\theta f+ \\phi)$$ </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1XGgjpuIwTx",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Sin_func:\n",
    "\n",
    "    \"\"\"\n",
    "    Class that computes the Sin function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_vector,\n",
    "        amp=[0, 1],\n",
    "        phase=[0, 1],\n",
    "        frequency=[0, 1],\n",
    "        size=(1, 1),\n",
    "        batch_size=1000,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x_vector:\n",
    "            sd (array, float): range for the standard deviation\n",
    "            mean (array, float): range for the mean\n",
    "            amp (array, float): range for the amplitude\n",
    "            size (tuple): Size of the array first index is number of channels, second is number of functions\n",
    "            verbose (bool): shows outputs\n",
    "        \"\"\"\n",
    "\n",
    "        self.x_vector = x_vector\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.amp = amp\n",
    "        self.amp_mean = torch.tensor(amp[0] + amp[1]) / 2\n",
    "        self.amp_sd = torch.sqrt(\n",
    "            torch.pow(torch.tensor(amp[1]) - torch.tensor(amp[0]), 2) / 12\n",
    "        )\n",
    "\n",
    "        self.phase = phase\n",
    "        self.phase_mean = torch.tensor(phase[0] + phase[1]) / 2\n",
    "        self.phase_sd = torch.sqrt(\n",
    "            torch.pow(torch.tensor(phase[1]) - torch.tensor(phase[0]), 2) / 12\n",
    "        )\n",
    "\n",
    "        self.frequency = frequency\n",
    "        self.frequency_mean = torch.tensor(frequency[0] + frequency[1]) / 2\n",
    "        self.frequency_sd = torch.sqrt(\n",
    "            torch.pow(torch.tensor(frequency[1]) - torch.tensor(frequency[0]), 2) / 12\n",
    "        )\n",
    "\n",
    "        self.size = size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def compute(self, params, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            self (object): Returns the instance itself.\n",
    "            device (string, optional) : Sets the device to do the computation. Default `cpu`, common option `cuda`\n",
    "\n",
    "        Returns: out (Tensor): spectra.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if len(params.size()) == 2:\n",
    "            params = torch.reshape(params, (params.shape[0], 3, -1))\n",
    "\n",
    "        out = torch.zeros(\n",
    "            (params.shape[0], self.x_vector.shape[0], self.size[0], self.size[1])\n",
    "        )\n",
    "\n",
    "        params = params.to(device)\n",
    "\n",
    "        for i in range(self.size[1]):\n",
    "\n",
    "            if params.ndim == 4:\n",
    "                _amp = params[:, 0, 0, i]\n",
    "                _phase = params[:, 0, 1, i]\n",
    "                _frequency = params[:, 0, 2, i]\n",
    "\n",
    "            if params.ndim == 3:\n",
    "                _amp = params[:, 0, i]\n",
    "                _phase = params[:, 1, i]\n",
    "                _frequency = params[:, 2, i]\n",
    "\n",
    "            x_vector = (\n",
    "                torch.cat(params.shape[0] * [self.x_vector])\n",
    "                .reshape(params.shape[0], -1)\n",
    "                .to(device)\n",
    "            )\n",
    "            x_vector = torch.transpose(x_vector, 0, 1)  # .to(device)\n",
    "\n",
    "            _out = _amp * torch.sin(\n",
    "                2 * torch.tensor(np.pi) * _frequency * x_vector + _phase\n",
    "            )\n",
    "\n",
    "            out[:, :, 0, i] = torch.transpose(_out, 0, 1)\n",
    "\n",
    "        return (torch.sum(out, dim=3), out)\n",
    "\n",
    "    def sampler(self, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            device (str): device where computation happens\n",
    "\n",
    "        Returns:\n",
    "            out (Tensor) : Generated spectra\n",
    "            params (Tensor) : parameters used for generation\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        phase = rand_tensor(\n",
    "            min=self.phase[0],\n",
    "            max=self.phase[1],\n",
    "            size=(self.batch_size, self.size[0], self.size[1]),\n",
    "        )\n",
    "        frequency = rand_tensor(\n",
    "            min=self.frequency[0],\n",
    "            max=self.frequency[1],\n",
    "            size=(self.batch_size, self.size[0], self.size[1]),\n",
    "        )\n",
    "        amp = rand_tensor(\n",
    "            min=self.amp[0],\n",
    "            max=self.amp[1],\n",
    "            size=(self.batch_size, self.size[0], self.size[1]),\n",
    "        )\n",
    "        _params = torch.torch.stack((amp, phase, frequency))\n",
    "\n",
    "        _params = torch.atleast_2d(_params)\n",
    "        _params = torch.transpose(_params, 0, 1)\n",
    "        _params = torch.transpose(_params, 1, 2)\n",
    "\n",
    "        return (self.compute(_params, device=device), _params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziSPN95kMH96",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "constructor = Sin_func(\n",
    "    amp=[0.2, 1],  # Sets the amplitude\n",
    "    phase=[0, 2 * np.pi],  # Sets the phase\n",
    "    frequency=[0.1, 0.5],  # Sets the frequency\n",
    "    x_vector=torch.linspace(0, np.pi, 100),  # Sets the x_vector\n",
    "    batch_size=10000,\n",
    ")  # number of samples to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLaP-ksVM-Vv",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# initializes the constructor\n",
    "output = constructor.sampler()\n",
    "\n",
    "# grabs the parameters and the spectra\n",
    "spectra, params = output\n",
    "\n",
    "# This grabs the sum of all spectral and the individual spectra if they exist\n",
    "spectra_full, spectras = spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fpr1aijpYuO",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualize examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "h7CLS6s0NLT4",
    "outputId": "88ed9e63-6940-41b3-cabf-1cdfce3a693c"
   },
   "outputs": [],
   "source": [
    "rand = np.random.randint(0, 10000)\n",
    "plt.plot(spectras[rand, :, 0].cpu(), \"b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HASHdT3YPCe3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Network Autoencoders\n",
    "\n",
    "- It is important to consider the temporal domain\n",
    "- This can be improved by using a recurrent neural network that processes each time step sequentially.\n",
    "- To add an understanding about the short and long term information in the data you can add memory and forget logic as a learnable parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HASHdT3YPCe3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://github.com/jagar2/AI_For_Atoms_Autoencoder_Tutorial/blob/main/img/Autoencoder_Med.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMyui0C1PCe3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://github.com/jagar2/AI_For_Atoms_Autoencoder_Tutorial/blob/main/img/LSTM%20Node.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfc1_DKg89Il",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Builds the autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6sCf2kDPCe3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=12):\n",
    "        self.latent_dim = latent_dim\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(1, 12, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(24, 12, batch_first=True, bidirectional=True)\n",
    "        self.embedding = nn.Linear(24, self.latent_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        (x, (_, __)) = self.lstm(x)\n",
    "        (x, (_, __)) = self.lstm2(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.embedding(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=12):\n",
    "        self.latent_dim = latent_dim\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(self.latent_dim, 12, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(24, 12, batch_first=True, bidirectional=True)\n",
    "        self.tdd = nn.Conv1d(24, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, :, None]\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x.repeat([1, 100, 1])\n",
    "        (x, (_, __)) = self.lstm(x)\n",
    "        (x, (_, __)) = self.lstm2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.tdd(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxBAeRAjn3j3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # encode\n",
    "\n",
    "        embedding = self.encoder(x)\n",
    "\n",
    "        # decode\n",
    "\n",
    "        predicted = self.decoder(embedding)\n",
    "\n",
    "        return predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaRlMbfttiaZ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since we know there are intrinsically 3 latent dimensions let's try and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mm-1aG9rPCe4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "latent_dim = 3\n",
    "\n",
    "encoder = Encoder(latent_dim=latent_dim).to(device)\n",
    "decoder = Decoder(latent_dim=latent_dim).to(device)\n",
    "model = Autoencoder(encoder, decoder).to(device)\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5t5hvfQPCe4",
    "outputId": "e17abdb6-4993-416c-c751-1e1b48cdcab0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# views the model\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Builds the dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0jldeW4OZSb",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# constructs a dataloader for training\n",
    "\n",
    "dataloader = DataLoader(spectra_full, batch_size=512, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eijf5211Qa7h",
    "outputId": "fb0e95a7-da1f-455c-d670-aa7c02c622ac",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# trains the model\n",
    "\n",
    "torch.manual_seed(0)\n",
    "Train(\n",
    "    model, encoder, decoder, dataloader, optimizer, 500,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfcYvUY4wjMz",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualize the reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8fpD4ncv29H",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# computes an example reconstruction for a mini batch\n",
    "\n",
    "(encoded_, decoded_) = transform_nn(next(iter(dataloader)), encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "NA20y3ijVcwc",
    "outputId": "8aa05dc5-c8cd-4a09-b119-70f58fe1e0c2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plots a random example of the original and predicted spectra\n",
    "\n",
    "rand = np.random.randint(0, 512)\n",
    "plt.plot(spectras[rand, :, 0].cpu(), \"b\", label=\"Original\")\n",
    "plt.plot(decoded_[rand].squeeze(), \"r\", label=\"Generated\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXjwNNaSz5MS",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generating Validation Data\n",
    "\n",
    "- We want to generate a hyperspectral image\n",
    "- This can be done by taking the RGB values of an image and using them as parameters for a function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJLKait50MP6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Loads and image of my dog Nala\n",
    "\n",
    "- Painting by _Irene Dogmatic_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MG4YZi2tfeSi",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Loads dog image\n",
    "\n",
    "image = io.imread(\n",
    "    \"https://github.com/jagar2/m3_learning/blob/main/m3_learning/Tutorials/Unsupervised_Learning_with_AEs/figs/nala.jpg?raw=true\"\n",
    ")\n",
    "\n",
    "# Crops dog image\n",
    "\n",
    "image = image[200:1900:20, 100:1500:20] / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUpSmdfP0Ysv",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Displays the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "ud8Peh1Sd9CJ",
    "outputId": "b0a04e29-83b9-4bc7-d276-ab523bde36eb",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generates the data from RGB sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBKSfOPExI3q",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Converts the image into parameters within the generated range\n",
    "\n",
    "nala_params = np.atleast_3d(image.reshape(-1, 3))\n",
    "\n",
    "nala_amp = torch.tensor(nala_params[:, 0, 0] * 0.8 + 0.2)\n",
    "nala_phase = torch.tensor(nala_params[:, 1, 0] * 2 * np.pi)\n",
    "nala_frequency = torch.tensor(nala_params[:, 2, 0] * 0.5 + 0.1)\n",
    "\n",
    "_nala_params = torch.torch.stack((nala_amp, nala_phase, nala_frequency))\n",
    "\n",
    "_nala_params = torch.atleast_3d(_nala_params)\n",
    "_nala_params = torch.transpose(_nala_params, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_zD6eocyTO8",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# builds the spectra from the parameters\n",
    "\n",
    "(nala_spectra, _) = constructor.compute(_nala_params)\n",
    "\n",
    "# generated the encoded representation and decoded spectra\n",
    "\n",
    "(nala_encoded_, nala_decoded_) = transform_nn(nala_spectra, encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "QIIx6V00yW2A",
    "outputId": "2e3117ae-5829-403a-90b3-72a2b3cd8f08",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plots a random example of the original and predicted spectra\n",
    "\n",
    "rand = np.random.randint(0, nala_spectra.shape[0])\n",
    "plt.plot(nala_spectra[rand, :, 0].cpu(), \"b\", label=\"Original\")\n",
    "plt.plot(nala_decoded_[rand].squeeze(), \"r\", label=\"Generated\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualize the learned results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "pTGCN2t0yfc5",
    "outputId": "cd8014ca-117d-4744-95d8-becce3f56df5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the learned embeddings\n",
    "\n",
    "embedding_maps(nala_encoded_, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "G3ebdrvlyWvg",
    "outputId": "1c1e864c-6bd3-492b-8cca-5f5058342917",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the actual RGB channels.\n",
    "\n",
    "embedding_maps(_nala_params.reshape(-1, 3), image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTptJUrR3ILQ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **There is minimal resemblance to the true features**\n",
    "\n",
    "- This is unsurprising because there are no rules that define what the embedding should look like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyeX8d_L3RoO",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's try a bigger model\n",
    "\n",
    "### Builds the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HD9AHYqh8rpD",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "latent_dim = 12\n",
    "\n",
    "encoder = Encoder(latent_dim=latent_dim).to(device)\n",
    "decoder = Decoder(latent_dim=latent_dim).to(device)\n",
    "model = Autoencoder(encoder, decoder).to(device)\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnCrCI4_8rpD",
    "outputId": "0534483a-a3e3-449c-ab2b-b612d3816cd9",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# views the model\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_DuA4Ta8rpE",
    "outputId": "ce01e347-efaa-4829-fbe6-2aca505e9f5b",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# trains the model\n",
    "\n",
    "torch.manual_seed(0)\n",
    "Train(\n",
    "    model, encoder, decoder, dataloader, optimizer, 500,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0rH1RJF8rpF",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualize the reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQiIfRNr8rpF",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# computes an example reconstruction for a minibatch\n",
    "\n",
    "(encoded_, decoded_) = transform_nn(next(iter(dataloader)), encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "Vw39umex8rpF",
    "outputId": "dd15aa3c-8691-4a82-c809-28c6b2cb76d6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plots a random example of the original and predicted spectra\n",
    "\n",
    "rand = np.random.randint(0, 512)\n",
    "plt.plot(spectras[rand, :, 0].cpu(), \"b\", label=\"Original\")\n",
    "plt.plot(decoded_[rand].squeeze(), \"r\", label=\"Generated\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9FvEzGbADuL",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Reconstruction is slightly better but just more overfit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualize the learned results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6Y5kHZx8rpH",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# builds the spectra from the parameters\n",
    "\n",
    "(nala_spectra, _) = constructor.compute(_nala_params)\n",
    "\n",
    "# generated the encoded representation and decoded spectra\n",
    "\n",
    "(nala_encoded_, nala_decoded_) = transform_nn(nala_spectra, encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "Eb1YE0RE8rpH",
    "outputId": "6b31b706-2507-46b5-f64c-069c3ea7cad9",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plots a random example of the original and predicted spectra\n",
    "\n",
    "rand = np.random.randint(0, nala_spectra.shape[0])\n",
    "plt.plot(nala_spectra[rand, :, 0].cpu(), \"b\", label=\"Original\")\n",
    "plt.plot(nala_decoded_[rand].squeeze(), \"r\", label=\"Generated\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "s2BSXlJC8rpH",
    "outputId": "179e9e48-f3b9-4673-85f7-e9e94479d8ea",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the learned embeddings\n",
    "\n",
    "embedding_maps(nala_encoded_, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "5FrPP3-Y8rpH",
    "outputId": "4a156602-00f5-4ee8-9f86-1356f43760e7",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the actual RGB channels.\n",
    "\n",
    "embedding_maps(_nala_params.reshape(-1, 3), image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEqt9hGT8rpI",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Now there are just more features but no resemblance between the parameters.**\n",
    "- It would be impossible to have any resemblance to the features since it is overcomplete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glpKkqPCAjIR",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Physics constrained neural network\n",
    "\n",
    "### Building the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VObnDnsnVcs2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class DensePhysLarger(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_vector,\n",
    "        model,\n",
    "        dense_params=3,\n",
    "        verbose=False,\n",
    "        device=\"cuda\",\n",
    "        num_channels=1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x_vector: The vector of values for x\n",
    "            model: the empirical function to fit\n",
    "            dense_params: number of output parameters to the model\n",
    "            verbose: sets if the model is verbose\n",
    "            device: device where the model will run\n",
    "            num_channels: number of channels in the input\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dense_params = dense_params\n",
    "        self.x_vector = x_vector\n",
    "        self.verbose = verbose\n",
    "        self.num_channels = num_channels\n",
    "        self.device = device\n",
    "        self.model_params = kwargs.get(\"model_params\")\n",
    "        self.model = model  # (self.x_vector, size=(num_channels, dense_params // self.model_params))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        n = 4\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "        # Input block of 1d convolution\n",
    "\n",
    "        self.hidden_x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.num_channels, out_channels=8 * n, kernel_size=7),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=8 * n, out_channels=6 * n, kernel_size=7),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=6 * n, out_channels=4, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        self.hidden_x1_shape = self.hidden_x1(\n",
    "            torch.zeros(1, self.num_channels, self.x_vector.shape[0])\n",
    "        ).shape\n",
    "\n",
    "        # fully connected block\n",
    "\n",
    "        self.hidden_xfc = nn.Sequential(\n",
    "            nn.Linear(self.hidden_x1_shape[1] * self.hidden_x1_shape[2], 20),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "        # out of size 20\n",
    "\n",
    "        self.hidden_xfc_shape = self.hidden_xfc(\n",
    "            torch.zeros(1, self.hidden_x1_shape[1] * self.hidden_x1_shape[2])\n",
    "        ).shape\n",
    "\n",
    "        # 2nd block of 1d-conv layers\n",
    "\n",
    "        self.hidden_x2 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=1, out_channels=4 * n, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4 * n, out_channels=4 * n, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4 * n, out_channels=4 * n, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4 * n, out_channels=4 * n, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4 * n, out_channels=4 * n, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.Conv1d(in_channels=4 * n, out_channels=4 * n, kernel_size=5),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=4 * n, out_channels=2 * n, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=2),\n",
    "            nn.Conv1d(in_channels=2 * n, out_channels=2, kernel_size=3),\n",
    "            nn.SELU(),\n",
    "            nn.AvgPool1d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.hidden_x2_shape = self.hidden_x2(\n",
    "            torch.zeros(\n",
    "                (\n",
    "                    self.hidden_xfc_shape[0],\n",
    "                    1,\n",
    "                    self.hidden_x1_shape[1] * self.hidden_x1_shape[2],\n",
    "                )\n",
    "            )\n",
    "        ).shape\n",
    "\n",
    "        # Flatten layer\n",
    "\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "\n",
    "        # Final embedding block - Output 4 values - linear\n",
    "\n",
    "        self.hidden_embedding = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                self.hidden_x2_shape[1] * self.hidden_x2_shape[2]\n",
    "                + self.hidden_xfc_shape[1],\n",
    "                16,\n",
    "            ),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(8, self.dense_params),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, n=-1):\n",
    "\n",
    "        x = self.hidden_x1(x)\n",
    "        xfc = torch.reshape(x, (x.shape[0], -1))  # batch size, features\n",
    "        xfc = self.hidden_xfc(xfc)\n",
    "        x = torch.reshape(\n",
    "            x, (x.shape[0], 1, self.hidden_x1_shape[1] * self.hidden_x1_shape[2])\n",
    "        )\n",
    "        x = self.hidden_x2(x)\n",
    "        cnn_flat = self.flatten_layer(x)\n",
    "        encoded = torch.cat((cnn_flat, xfc), 1)  # merge dense and 1d conv.\n",
    "\n",
    "        embedding = self.hidden_embedding(encoded)  # output is 3 parameters\n",
    "\n",
    "        embedding = torch.reshape(embedding, (embedding.shape[0], 3, -1))\n",
    "\n",
    "        embedding[:, 0, :] = (\n",
    "            embedding[:, 0, :] * self.model.amp_sd + self.model.amp_mean\n",
    "        )\n",
    "        embedding[:, 1, :] = (\n",
    "            embedding[:, 1, :] * self.model.phase_sd + self.model.phase_mean\n",
    "        )\n",
    "        embedding[:, 2, :] = (\n",
    "            embedding[:, 2, :] * self.model.frequency_sd + self.model.frequency_mean\n",
    "        )\n",
    "\n",
    "        embedding = torch.reshape(embedding, (embedding.shape[0], -1))\n",
    "\n",
    "        embedding = torch.abs(embedding)\n",
    "        self.embed = embedding\n",
    "\n",
    "        (out, _) = self.model.compute(embedding, device=self.device)\n",
    "\n",
    "        out = torch.transpose(out, 1, 2)\n",
    "        out = torch.atleast_3d(out)\n",
    "\n",
    "        return (out.to(self.device), embedding.to(self.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKrvL1bxVcjn",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_vector = torch.linspace(0, 10, 100)\n",
    "\n",
    "model = DensePhysLarger(\n",
    "    x_vector, constructor, dense_params=3, model_params=3, verbose=False\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4QtxjDaXm0s",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# builds the dataloader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    torch.transpose(spectra_full, 1, 2), batch_size=512, shuffle=True, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDHXFqcjfNy5",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "set_seeds(42)\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for train_batch in dataloader:\n",
    "        pred, _ = model(train_batch.cuda())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_func(train_batch.cuda(), pred)\n",
    "        loss.backward(create_graph=True)\n",
    "        train_loss += loss.item() * pred.shape[0]\n",
    "        total_num += pred.shape[0]\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= total_num\n",
    "\n",
    "    print(\"epoch : {}/{}, recon loss = {:.8f}\".format(epoch, epochs, train_loss))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "bzpudYqD1uT6",
    "outputId": "55eef489-65e7-4bfb-bbe6-51194d44051e",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spectra_generated, params = model(train_batch.cuda())\n",
    "rand = np.random.randint(0, 272)\n",
    "plt.plot(spectra_generated[rand, 0, :].detach().cpu().numpy(), \"r\")\n",
    "plt.plot(train_batch[rand, 0, :], \"b\")\n",
    "print(params[rand])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "o4YyAVHNSrQR",
    "outputId": "e1384f5f-1642-41e9-b990-8208dae26c5a",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nala_spectra_generated, nala_params = model(nala_spectra.transpose(2, 1).cuda())\n",
    "rand = np.random.randint(0, nala_spectra_generated.shape[0])\n",
    "plt.plot(nala_spectra_generated[rand, 0, :].detach().cpu().numpy(), \"r\")\n",
    "plt.plot(nala_spectra[rand, :, 0], \"b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtbZ7_hWqi-j",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# removes 2pi shifts\n",
    "nala_params[:, 1] = nala_params[:, 1] % 2 * np.pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "HBWPz48wpRKJ",
    "outputId": "1b681f09-4294-4350-c852-7ce3523dcdac",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the learned embeddings\n",
    "\n",
    "embedding_maps(nala_params.detach().cpu().numpy(), image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "PlpTqUciqLcM",
    "outputId": "958b2239-a441-4bdf-c72e-9be17364b0ee",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the actual RGB channels.\n",
    "\n",
    "embedding_maps(_nala_params.reshape(-1, 3), image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **results are much closer to the underlying physics since we enforced them**\n",
    "- The middle parameter is the phase. This is the hardest to learn $\\rightarrow$ this makes sense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqWzl7gZsYTE",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Try with a better optimizer AdaHessian\n",
    "\n",
    "- There are better optimizers than ADAM that use second-order information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JL5Xx1J-qXQW",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sun Feb 26 16:34:00 2021\n",
    "@author: Amir Gholami\n",
    "@coauthor: David Samuel\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class AdaHessian(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Implements the AdaHessian algorithm from \"ADAHESSIAN: An Adaptive Second OrderOptimizer for Machine Learning\"\n",
    "    Arguments:\n",
    "        params (iterable) -- iterable of parameters to optimize or dicts defining parameter groups\n",
    "        lr (float, optional) -- learning rate (default: 0.1)\n",
    "        betas ((float, float), optional) -- coefficients used for computing running averages of gradient and the squared hessian trace (default: (0.9, 0.999))\n",
    "        eps (float, optional) -- term added to the denominator to improve numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional) -- weight decay (L2 penalty) (default: 0.0)\n",
    "        hessian_power (float, optional) -- exponent of the hessian trace (default: 1.0)\n",
    "        update_each (int, optional) -- compute the hessian trace approximation only after *this* number of steps (to save time) (default: 1)\n",
    "        n_samples (int, optional) -- how many times to sample `z` for the approximation of the hessian trace (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=0.1,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8,\n",
    "        weight_decay=0.0,\n",
    "        hessian_power=1.0,\n",
    "        update_each=1,\n",
    "        n_samples=1,\n",
    "        average_conv_kernel=False,\n",
    "    ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n",
    "        if not 0.0 <= hessian_power <= 1.0:\n",
    "            raise ValueError(f\"Invalid Hessian power value: {hessian_power}\")\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "        self.update_each = update_each\n",
    "        self.average_conv_kernel = average_conv_kernel\n",
    "\n",
    "        # use a separate generator that deterministically generates the same `z`s across all GPUs in case of distributed training\n",
    "        self.generator = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            betas=betas,\n",
    "            eps=eps,\n",
    "            weight_decay=weight_decay,\n",
    "            hessian_power=hessian_power,\n",
    "        )\n",
    "        super(AdaHessian, self).__init__(params, defaults)\n",
    "\n",
    "        for p in self.get_params():\n",
    "            p.hess = 0.0\n",
    "            self.state[p][\"hessian step\"] = 0\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Gets all parameters in all param_groups with gradients\n",
    "        \"\"\"\n",
    "\n",
    "        return (\n",
    "            p for group in self.param_groups for p in group[\"params\"] if p.requires_grad\n",
    "        )\n",
    "\n",
    "    def zero_hessian(self):\n",
    "        \"\"\"\n",
    "        Zeros out the accumulated hessian traces.\n",
    "        \"\"\"\n",
    "\n",
    "        for p in self.get_params():\n",
    "            if (\n",
    "                not isinstance(p.hess, float)\n",
    "                and self.state[p][\"hessian step\"] % self.update_each == 0\n",
    "            ):\n",
    "                p.hess.zero_()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def set_hessian(self):\n",
    "        \"\"\"\n",
    "        Computes the Hutchinson approximation of the hessian trace and accumulates it for each trainable parameter.\n",
    "        \"\"\"\n",
    "\n",
    "        params = []\n",
    "        for p in filter(lambda p: p.grad is not None, self.get_params()):\n",
    "            if (\n",
    "                self.state[p][\"hessian step\"] % self.update_each == 0\n",
    "            ):  # compute the trace only for each `update_each` step\n",
    "                params.append(p)\n",
    "            self.state[p][\"hessian step\"] += 1\n",
    "\n",
    "        if len(params) == 0:\n",
    "            return\n",
    "\n",
    "        if (\n",
    "            self.generator.device != params[0].device\n",
    "        ):  # hackish way of casting the generator to the right device\n",
    "            self.generator = torch.Generator(params[0].device).manual_seed(2147483647)\n",
    "\n",
    "        grads = [p.grad for p in params]\n",
    "\n",
    "        for i in range(self.n_samples):\n",
    "            zs = [\n",
    "                torch.randint(0, 2, p.size(), generator=self.generator, device=p.device)\n",
    "                * 2.0\n",
    "                - 1.0\n",
    "                for p in params\n",
    "            ]  # Rademacher distribution {-1.0, 1.0}\n",
    "            h_zs = torch.autograd.grad(\n",
    "                grads,\n",
    "                params,\n",
    "                grad_outputs=zs,\n",
    "                only_inputs=True,\n",
    "                retain_graph=i < self.n_samples - 1,\n",
    "            )\n",
    "            for h_z, z, p in zip(h_zs, zs, params):\n",
    "                p.hess += (\n",
    "                    h_z * z / self.n_samples\n",
    "                )  # approximate the expected values of z*(H@z)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional) -- a closure that reevaluates the model and returns the loss (default: None)\n",
    "        \"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        self.zero_hessian()\n",
    "        self.set_hessian()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None or p.hess is None:\n",
    "                    continue\n",
    "\n",
    "                if self.average_conv_kernel and p.dim() == 4:\n",
    "                    p.hess = (\n",
    "                        torch.abs(p.hess)\n",
    "                        .mean(dim=[2, 3], keepdim=True)\n",
    "                        .expand_as(p.hess)\n",
    "                        .clone()\n",
    "                    )\n",
    "\n",
    "                # Perform correct stepweight decay as in AdamW\n",
    "                p.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 1:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(\n",
    "                        p.data\n",
    "                    )  # Exponential moving average of gradient values\n",
    "                    state[\"exp_hessian_diag_sq\"] = torch.zeros_like(\n",
    "                        p.data\n",
    "                    )  # Exponential moving average of Hessian diagonal square values\n",
    "\n",
    "                exp_avg, exp_hessian_diag_sq = (\n",
    "                    state[\"exp_avg\"],\n",
    "                    state[\"exp_hessian_diag_sq\"],\n",
    "                )\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(p.grad, alpha=1 - beta1)\n",
    "                exp_hessian_diag_sq.mul_(beta2).addcmul_(\n",
    "                    p.hess, p.hess, value=1 - beta2\n",
    "                )\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state[\"step\"]\n",
    "                bias_correction2 = 1 - beta2 ** state[\"step\"]\n",
    "\n",
    "                k = group[\"hessian_power\"]\n",
    "                denom = (\n",
    "                    (exp_hessian_diag_sq / bias_correction2)\n",
    "                    .pow_(k / 2)\n",
    "                    .add_(group[\"eps\"])\n",
    "                )\n",
    "\n",
    "                # make update\n",
    "                step_size = group[\"lr\"] / bias_correction1\n",
    "                p.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Builds the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxv9kJIOsyq0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_vector = torch.linspace(0, 10, 100)\n",
    "\n",
    "model = DensePhysLarger(\n",
    "    x_vector, constructor, dense_params=3, model_params=3, verbose=False\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJwjRvbksyq0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# builds the dataloader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    torch.transpose(spectra_full, 1, 2), batch_size=512, shuffle=True, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-0ntsQZshWM",
    "outputId": "a0561ed6-4362-417b-d42a-5347dc4ae7ad",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "set_seeds(42)\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "# Use AdaHessian\n",
    "\n",
    "optimizer = AdaHessian(model.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    total_num = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for train_batch in dataloader:\n",
    "        pred, _ = model(train_batch.cuda())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_func(train_batch.cuda(), pred)\n",
    "        loss.backward(create_graph=True)\n",
    "        train_loss += loss.item() * pred.shape[0]\n",
    "        total_num += pred.shape[0]\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= total_num\n",
    "\n",
    "    print(\"epoch : {}/{}, recon loss = {:.8f}\".format(epoch, epochs, train_loss))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yN_yCo6nshWM",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spectra_generated, params = model(train_batch.cuda())\n",
    "rand = np.random.randint(0, 272)\n",
    "plt.plot(spectra_generated[rand, 0, :].detach().cpu().numpy(), \"r\")\n",
    "plt.plot(train_batch[rand, 0, :], \"b\")\n",
    "print(params[rand])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpYzwfYDshWM",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nala_spectra_generated, nala_params = model(nala_spectra.transpose(2, 1).cuda())\n",
    "rand = np.random.randint(0, nala_spectra_generated.shape[0])\n",
    "plt.plot(nala_spectra_generated[rand, 0, :].detach().cpu().numpy(), \"r\")\n",
    "plt.plot(nala_spectra[rand, :, 0], \"b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLc33BIqshWN",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nala_params[:, 1] = nala_params[:, 1] % 2 * np.pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6_bQBfVshWN",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the learned embeddings\n",
    "\n",
    "embedding_maps(nala_params.detach().cpu().numpy(), image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfpYeDHhshWN",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the actual RGB channels.\n",
    "\n",
    "embedding_maps(_nala_params.reshape(-1, 3), image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly the best result\n",
    "\n",
    "- It is quite impressive that we can build a feed forward model to fit data to complex functions\n",
    "- This is actually a very hard task for a neural network as frequency and phase are something that cannot be learned easily in convolutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Autoencoder Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('m3_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d82ed07f1a000548bae641f7bea0b50abc9c5d353fc085ff35eceda152964b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
